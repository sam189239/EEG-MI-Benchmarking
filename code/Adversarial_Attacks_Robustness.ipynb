{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Attacks:\n",
        "\n",
        "1. **Additive White Gaussian Noise (AWGN) Attack**: This attack involves adding carefully crafted noise to the input EEG signals to make them misclassified by the model. The added noise is usually imperceptible to humans but can significantly affect the model's performance.\n",
        "   - White noise can be generated such that it has a mean of 0 and a specified standard deviation.\n",
        "   - Noise level: You should experiment with different levels to understand at what point the model's performance starts to degrade.\n",
        "   - Applying AWGN to EEG Signals\n"
      ],
      "metadata": {
        "id": "N9UBxCfjymbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xlyV4nRwnXy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def awgn_attack(signal, noise_level):\n",
        "    mean = 0\n",
        "    std_dev = noise_level\n",
        "    white_noise = np.random.normal(mean, std_dev, signal.shape)\n",
        "    return signal + white_noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Fast Gradient Sign Method (FGSM)**: FGSM is a popular adversarial attack that involves perturbing the input EEG signals in the direction of the gradient of the loss function with respect to the input. This results in small, but adversarially meaningful, perturbations that can cause misclassification by the model.\n",
        "   - Need access to the model's parameters and the ability to compute gradients, which is possible in frameworks like TensorFlow or PyTorch.\n",
        "   - Applying the FGSM Perturbation: The perturbation is computed as the sign of the gradient multiplied by a small factor (ε, epsilon). This factor controls the magnitude of the perturbation.\n",
        "   - Using PyTorch,\n"
      ],
      "metadata": {
        "id": "ftdMEuQMy-XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def fgsm_attack(model, loss_fn, inputs, labels, epsilon):\n",
        "    # Ensure model's parameters won't update\n",
        "    inputs.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass to get gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Get the sign of the gradients\n",
        "    data_grad = inputs.grad.data\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_input = inputs + epsilon * data_grad.sign()\n",
        "\n",
        "    # Adding clipping to maintain [0,1] range if necessary\n",
        "    perturbed_input = torch.clamp(perturbed_input, 0, 1)\n",
        "\n",
        "    return perturbed_input\n"
      ],
      "metadata": {
        "id": "8Ls2M9EUzBJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Projected Gradient Descent (PGD)**: PGD is an iterative version of FGSM where the perturbations are applied multiple times with small step sizes while ensuring that the perturbed signals remain within a specified epsilon-ball around the original signals. PGD tends to produce more effective adversarial examples compared to FGSM.\n",
        "   - PGD is like FGSM but applies the perturbation in multiple small steps. This iterative approach allows PGD to explore more of the input space around the original data.\n",
        "   - Algorithm:\n",
        "     - Start with a randomly perturbed version of the original input within the ε-ball\n",
        "     - In each iteration, apply a small FGSM-like step, and then clip the result to ensure it stays within the ε-ball.\n",
        "     - Repeat for a fixed number of iterations or until convergence.\n",
        "   - Using PyTorch,\n"
      ],
      "metadata": {
        "id": "wiwWSoXfzFMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def pgd_attack(model, inputs, labels, epsilon, alpha, num_iter):\n",
        "    perturbed_inputs = inputs.clone()\n",
        "    perturbed_inputs = perturbed_inputs + torch.rand_like(inputs) * 2 * epsilon - epsilon\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        perturbed_inputs.requires_grad = True\n",
        "        outputs = model(perturbed_inputs)\n",
        "        model.zero_grad()\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # FGSM step\n",
        "            step = alpha * perturbed_inputs.grad.sign()\n",
        "            perturbed_inputs = perturbed_inputs + step\n",
        "\n",
        "            # Clipping step (project back to ε-ball)\n",
        "            perturbed_inputs = torch.max(torch.min(perturbed_inputs, inputs + epsilon), inputs - epsilon)\n",
        "\n",
        "    return perturbed_inputs\n"
      ],
      "metadata": {
        "id": "NQUiNA97zOPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Jacobian-based Saliency Map Attack (JSMA)**: JSMA identifies the most influential features (or electrodes in the case of EEG signals) and perturbs them to maximize the misclassification probability. It's based on computing the Jacobian matrix of the model's output with respect to the input features.\n",
        "   - The attack is based on the concept of a saliency map, which is derived from the Jacobian matrix of the model's output with respect to the input.\n",
        "   - Creating a saliency map:\n",
        "     1. Using the Jacobian matrix, a saliency map is created that highlights which features should be perturbed to most effectively change the model's output.\n",
        "     2. The goal is to find the features whose modification will either maximize the probability of a specific (incorrect) class or minimize the probability of the correct class.\n",
        "   - Implementation involves computing the Jacobian matrix and then iteratively modifying the most influential features according to the saliency map.\n",
        "   - Using PyTorch,\n"
      ],
      "metadata": {
        "id": "LJsJnP17zSeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified conceptual Python code\n",
        "def jsma_attack(model, input_features, target_label, max_distortion):\n",
        "    for i in range(max_distortion):\n",
        "        jacobian = compute_jacobian(model, input_features)\n",
        "        saliency_map = calculate_saliency_map(jacobian, target_label)\n",
        "        most_influential_feature = identify_most_influential_feature(saliency_map)\n",
        "        input_features = modify_feature(input_features, most_influential_feature)\n",
        "        if model.predict(input_features) == target_label:\n",
        "            break\n",
        "    return input_features\n"
      ],
      "metadata": {
        "id": "Uu51VtkRzSww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **DeepFool Attack**: DeepFool is an iterative attack that aims to find the minimum perturbation required to cause misclassification by iteratively linearizing the decision boundary of the model and moving the input signals towards the decision boundary.\n",
        "   - The key idea of DeepFool is to iteratively push the input data towards the model's decision boundary until it crosses into a different classification region, causing misclassification.\n",
        "   - It tries to calculate the minimal perturbation required to change the classification, which gives an estimate of the model's robustness.\n",
        "   - Iterative Linearization:\n",
        "     - At each iteration, DeepFool approximates the model's decision boundary near the current data point as linear.\n",
        "     - It then calculates the minimal perturbation needed to reach this linearized decision boundary.\n",
        "   - Moving Towards the Decision Boundary:\n",
        "     - The process is repeated iteratively until the altered input crosses the boundary, resulting in a different classification.\n",
        "   - Using PyTorch,\n"
      ],
      "metadata": {
        "id": "slEh--TRzYed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode for DeepFool\n",
        "def deepfool_attack(model, input_data, num_classes):\n",
        "    perturbed_data = input_data.clone()\n",
        "    for _ in range(max_iterations):\n",
        "        gradients = compute_gradients(model, perturbed_data, num_classes)\n",
        "        perturbation = calculate_minimal_perturbation(gradients)\n",
        "        perturbed_data += perturbation\n",
        "        if model.predict(perturbed_data) != model.predict(input_data):\n",
        "            break\n",
        "    return perturbed_data\n"
      ],
      "metadata": {
        "id": "KLQ8kuygzY6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **CleverHans Library**: CleverHans is a Python library for benchmarking adversarial attacks against machine learning models. It provides implementations of various adversarial attacks, including FGSM, PGD, and JSMA, making it a useful tool for experimenting with different attack strategies on EEG classification models.\n",
        "   - Install cleverhans, `pip install cleverhans`\n",
        "   - CleverHans offers a range of functions to implement various adversarial attacks\n",
        "   - Choose the attack you want to apply, such as FGSM, PGD, or JSMA. Configure the parameters of the attack according to your testing needs.\n",
        "   - Using cleverhans for attacks:\n",
        "     - For example, to use FGSM, you would set up the attack with the appropriate parameters like epsilon (the attack strength).\n",
        "     - Generate the adversarial examples using your EEG data as inputs.\n"
      ],
      "metadata": {
        "id": "3_cfdQQ5zeot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cleverhans"
      ],
      "metadata": {
        "id": "TYodlI2EzhjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "# Assuming `model` is your EEG classifier and `x_test` is the input data\n",
        "fgsm_params = {'epsilon': 0.3, 'clip_min': 0., 'clip_max': 1.}\n",
        "adv_x = fast_gradient_method(model, x_test, **fgsm_params)\n"
      ],
      "metadata": {
        "id": "1B84MXwMzeWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robustness:\n",
        "\n",
        "- **Noise Addition**:\n",
        "  - White Noise: Random noise that has equal intensity at different frequencies, providing a constant power spectral density.\n",
        "\n",
        "You can generate white noise programmatically using random functions in most data analysis software.\n"
      ],
      "metadata": {
        "id": "qlXRIiXv0SLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: Adding white noise to EEG data\n",
        "def add_white_noise(eeg_data, noise_level=0.1):\n",
        "    # Generate white noise\n",
        "    white_noise = np.random.normal(0, noise_level, eeg_data.shape)\n",
        "    # Add noise to the EEG data\n",
        "    noisy_eeg = eeg_data + white_noise\n",
        "    return noisy_eeg\n"
      ],
      "metadata": {
        "id": "-sIqvglw0R-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data Perturbation Techniques**:\n",
        "  - Time Warping: Slightly alter the time axis of the EEG signals to simulate variations in signal speed.\n",
        "    - Methods: Expand or reduce the time axis\n",
        "    - Python: numpy or scipy\n",
        "  - Warping factors: A warping factor > 1 stretches the signal (slows it down), and < 1 compresses it (speeds it up)\n"
      ],
      "metadata": {
        "id": "8gwLcqPm0R0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def time_warp(signal, warping_factor):\n",
        "    n = len(signal)\n",
        "    original_time = np.linspace(0, 1, n)\n",
        "    warped_time = np.linspace(0, 1, int(warping_factor * n))\n",
        "\n",
        "    interpolator = interp1d(original_time, signal, kind='linear')\n",
        "    warped_signal = interpolator(warped_time)\n",
        "\n",
        "    return warped_signal\n",
        "\n",
        "# Example usage\n",
        "eeg_signal = np.sin(np.linspace(0, 10*np.pi, 1000))  # A sample EEG-like signal\n",
        "warped_signal = time_warp(eeg_signal, 0.8)  # Warping factor < 1 compresses the signal\n",
        "\n",
        "plt.plot(eeg_signal, label='Original Signal')\n",
        "plt.plot(warped_signal, label='Warped Signal')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lVkYlA-v0RmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}